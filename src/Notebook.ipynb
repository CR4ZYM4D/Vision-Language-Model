{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a118f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34229a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension = 768 \n",
    "\n",
    "linear_dimension = 3072 # dimension of the linear layer of feed forward for normalization\n",
    "\n",
    "image_size = 224 # pixels in each dimension of image\n",
    "\n",
    "num_channels = 3 # number of channels in image (R, G, B)\n",
    "\n",
    "patch_size = 16 # number of patches/ blocks we break each image into\n",
    "\n",
    "attention_heads = 12 # number of attention heads\n",
    "\n",
    "num_layers = 12 # number of transformer block layers in the model\n",
    "\n",
    "dropout = 0.4\n",
    "\n",
    "normalization_constant = 1e-6 # proportionality constant for normalization layer\n",
    "\n",
    "num_image_tokens : int = None # number of tokens produced for each image i.e. it produces a list of vectors/ embeddings for a patch of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52497ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the embedding layer of the vision transformer model\n",
    "\n",
    "class VisionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension, patch_size, num_channels, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutor = nn.Conv2d(\n",
    "            in_channels = num_channels,  # 3 channels per image R, G, B\n",
    "            out_channels= vector_dimension, \n",
    "            kernel_size= patch_size,\n",
    "            stride = patch_size\n",
    "            )\n",
    "        \n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.positional_embedding = nn.Embedding(self.num_patches, vector_dimension)\n",
    "\n",
    "        self.register_buffer(\"positions\", torch.arange(self.num_patches).expand((1, -1)), persistent= False)\n",
    "\n",
    "    def forward(self, image_tensors):\n",
    "\n",
    "        # image tensors are of shape [ batch_size x num_channels x height x width ] \n",
    "\n",
    "        # converting them to tensors of shape [ batch_size x vector_dimension x (height / patch_size) x (width / patch_size)]\n",
    "        # using the CNN\n",
    "\n",
    "        image_embeddings = self.convolutor(image_tensors) # [ batch_size x vector_dimension x (height / patch_size) x (width / patch_size) ]\n",
    "\n",
    "        image_embeddings = image_embeddings.flatten(2) # [ batch_size x vector_dimension x num_patches ]\n",
    "\n",
    "        image_embeddings = image_embeddings.transpose(1, 2) # [ batch_size x num_patches x vector_dimension ]\n",
    "\n",
    "        return image_embeddings + self.positional_embedding(self.positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c09a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVisionTransformer\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vector_dimension, normalization_constant):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension, normalization_constant, patch_size, num_channels, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = VisionEmbedding(vector_dimension, patch_size, num_channels, image_size)\n",
    "\n",
    "        self.encoder_layer\n",
    "\n",
    "        self.post_normalization_layer = nn.LayerNorm(vector_dimension, normalization_constant)\n",
    "\n",
    "    def forward(self, image_data):\n",
    "\n",
    "        hidden_state = self.embedding_layer(image_data)\n",
    "\n",
    "        hidden_state = self.encoder_layer(hidden_state)\n",
    "\n",
    "        hidden_state = self.post_normalization_layer(hidden_state)\n",
    "\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea665e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
