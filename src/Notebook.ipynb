{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a118f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34229a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_dimension = 768 \n",
    "\n",
    "linear_dimension = 3072 # dimension of the linear layer of feed forward for normalization\n",
    "\n",
    "image_size = 224 # pixels in each dimension of image\n",
    "\n",
    "num_channels = 3 # number of channels in image (R, G, B)\n",
    "\n",
    "patch_size = 16 # number of patches/ blocks we break each image into\n",
    "\n",
    "attention_heads = 12 # number of attention heads\n",
    "\n",
    "num_layers = 12 # number of transformer block layers in the model\n",
    "\n",
    "dropout = 0.4\n",
    "\n",
    "normalization_constant = 1e-6 # proportionality constant for normalization layer\n",
    "\n",
    "num_image_tokens : int = None # number of tokens produced for each image i.e. it produces a list of vectors/ embeddings for a patch of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c5003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52497ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the embedding layer of the vision transformer model\n",
    "\n",
    "class VisionEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension, patch_size, num_channels, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convolutor = nn.Conv2d(\n",
    "            in_channels = num_channels,  # 3 channels per image R, G, B\n",
    "            out_channels= vector_dimension, \n",
    "            kernel_size= patch_size,\n",
    "            stride = patch_size\n",
    "            )\n",
    "        \n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.positional_embedding = nn.Embedding(self.num_patches, vector_dimension)\n",
    "\n",
    "        self.register_buffer(\"positions\", torch.arange(self.num_patches).expand((1, -1)), persistent= False)\n",
    "\n",
    "    def forward(self, image_tensors):\n",
    "\n",
    "        # image tensors are of shape [ batch_size x num_channels x height x width ] \n",
    "\n",
    "        # converting them to tensors of shape [ batch_size x vector_dimension x (height / patch_size) x (width / patch_size)]\n",
    "        # using the CNN\n",
    "\n",
    "        image_embeddings = self.convolutor(image_tensors) # [ batch_size x vector_dimension x (height / patch_size) x (width / patch_size) ]\n",
    "\n",
    "        image_embeddings = image_embeddings.flatten(2) # [ batch_size x vector_dimension x num_patches ]\n",
    "\n",
    "        image_embeddings = image_embeddings.transpose(1, 2) # [ batch_size x num_patches x vector_dimension ]\n",
    "\n",
    "        return image_embeddings + self.positional_embedding(self.positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the Vision Encoder block MLP layer\n",
    "\n",
    "class VisionMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension, linear_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(vector_dimension, linear_dimension)\n",
    "        self.layer2 = nn.Linear(linear_dimension, vector_dimension)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "\n",
    "        hidden_state = self.layer1(hidden_state)\n",
    "\n",
    "        hidden_state = f.gelu(hidden_state, approximate= \"tanh\")\n",
    "\n",
    "        return self.layer2(hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b548750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the attention Block. Since it is an image transformer and not a langugae one, there is no need of a causal mask\n",
    "\n",
    "class VisionAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension, attention_heads, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = attention_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.vector_dimension = vector_dimension\n",
    "        self.head_dimension = vector_dimension // attention_heads\n",
    "\n",
    "        self.wQ = nn.Linear(vector_dimension, vector_dimension) # Query projection layer\n",
    "        self.wK = nn.Linear(vector_dimension, vector_dimension) # Key projection layer\n",
    "        self.wV = nn.Linear(vector_dimension, vector_dimension) # Value projection layer\n",
    "        self.wO = nn.Linear(vector_dimension, vector_dimension) # Output projection layer\n",
    "\n",
    "        self.scale = vector_dimension ** -0.5\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "\n",
    "        # hidden_state [ batch_size x num_patches x vector_dimension ]\n",
    "        batch_size, sequence_len = hidden_state.size() # sequence len = num_patches\n",
    "\n",
    "        queryProjection = self.wQ(hidden_state) # [ batch_size x num_patches x vector_dimension ]\n",
    "        keyProjection = self.wQ(hidden_state) # [ batch_size x num_patches x vector_dimension ]\n",
    "        valueProjection = self.wQ(hidden_state) # [ batch_size x num_patches x vector_dimension ]\n",
    "\n",
    "        queryProjection = queryProjection.view(batch_size, sequence_len, self.num_heads, self.head_dimension) # [batch_size x sequence_len x num_heads x head_dimension]\n",
    "        keyProjection = keyProjection.view(batch_size, sequence_len, self.num_heads, self.head_dimension) # [batch_size x sequence_len x num_heads x head_dimension]\n",
    "        valueProjection = valueProjection.view(batch_size, sequence_len, self.num_heads, self.head_dimension) # [batch_size x sequence_len x num_heads x head_dimension]\n",
    "\n",
    "        queryProjection = queryProjection.transpose(1, 2) # [batch_size x num_heads x sequence_len x head_dimension]\n",
    "        keyProjection = keyProjection.transpose(1, 2) # [batch_size x num_heads x sequence_len x head_dimension]\n",
    "        valueProjection = valueProjection.transpose(1, 2) # [batch_size x num_heads x sequence_len x head_dimension]\n",
    "\n",
    "        attention_scores = torch.matmul(queryProjection, keyProjection.transpose(2, 3)) * self.scale # [batch_size x num_heads x sequence_len x sequence_len]\n",
    "\n",
    "        attention_scores = f.softmax(attention_scores, dim = -1)\n",
    "\n",
    "        attention_scores = torch.matmul(attention_scores, valueProjection) # [ batch_size x num_heads x  sequence_len x head_dimension]\n",
    "\n",
    "        attention_scores = attention_scores.transpose(1, 2).contiguous() # [batch_size x sequence_len x num_heads x head_dimension]\n",
    "\n",
    "        attention_scores = attention_scores.reshape(batch_size, sequence_len, self.vector_dimension) # [batch_size, sequence_len, vector_dimension]\n",
    "\n",
    "        return self.wO(attention_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for a single encoder block of the vision transformer\n",
    "\n",
    "class VisionEncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, ):\n",
    "        super().__init__(vector_dimension, normalization_constant, attention_heads, dropout, linear_dimension)\n",
    "\n",
    "        self.layerNorm1 = nn.LayerNorm(vector_dimension, normalization_constant)\n",
    "        self.layerNorm2 = nn.LayerNorm(vector_dimension, normalization_constant) \n",
    "\n",
    "        self.attentionBlock\n",
    "\n",
    "        self.MLP \n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "\n",
    "        skip_connector = torch.clone(hidden_states)\n",
    "\n",
    "        hidden_states = self.layerNorm1(hidden_states)\n",
    "\n",
    "        hidden_states = self.attentionBlock()\n",
    "\n",
    "        hidden_states = hidden_states + skip_connector\n",
    "\n",
    "        skip_connector = hidden_states\n",
    "\n",
    "        hidden_states = self.layerNorm2(hidden_states)\n",
    "\n",
    "        return hidden_states + skip_connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8c09a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVisionTransformer\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vector_dimension, normalization_constant):\n\u001b[32m      4\u001b[39m         \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension, normalization_constant, patch_size, num_channels, image_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = VisionEmbedding(vector_dimension, patch_size, num_channels, image_size)\n",
    "\n",
    "        self.encoder_layer\n",
    "\n",
    "        self.post_normalization_layer = nn.LayerNorm(vector_dimension, normalization_constant)\n",
    "\n",
    "    def forward(self, image_data):\n",
    "\n",
    "        hidden_state = self.embedding_layer(image_data)\n",
    "\n",
    "        hidden_state = self.encoder_layer(hidden_state)\n",
    "\n",
    "        hidden_state = self.post_normalization_layer(hidden_state)\n",
    "\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea665e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
